{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Getting reviews dataset and saving it.\n",
    "URL= 'https://store.steampowered.com/appreviews/285190'\n",
    "cursor_string='*'\n",
    "cursor_list=['']\n",
    "review_data= pd.DataFrame()\n",
    "parameters={'json': 1,'filter': 'all','language': 'english','day_range': 5000,'cursor': cursor_string,\n",
    "            'review_type': 'all','purchase_type': 'all','num_per_page': 1}\n",
    "#we access the reviews the first time in order to ascertain how many reviews there are to download\n",
    "response= requests.get( url= URL, params=parameters)\n",
    "raw_data=response.json()\n",
    "summary=raw_data['query_summary']\n",
    "\n",
    "#save the summary as a txt file\n",
    "summaryString= json.dumps(summary)\n",
    "with open('SteamData/SteamReviewSummary.txt', 'w') as file:\n",
    "    file.write(summaryString)\n",
    "    \n",
    "#set the maximum number of reviews per request\n",
    "parameters['num_per_page']=100\n",
    "#calculate how many times to loop the get request, based on the total number of reviews in our query summary\n",
    "request_iterations =1+int(summary['total_reviews']/parameters['num_per_page'])\n",
    "\n",
    "for i in range(request_iterations):\n",
    "    response= requests.get( url= URL, params=parameters)\n",
    "    raw_data=response.json()\n",
    "    raw_reviews=raw_data['reviews']\n",
    "    review_data=review_data.append(pd.json_normalize(raw_reviews, max_level = 2))\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    cursor_list+=cursor_string\n",
    "    cursor_string=raw_data['cursor']\n",
    "    parameters['cursor']= cursor_string\n",
    "    \n",
    "    #get the last remaining set of reviews (not included in the 100 per page iterations)\n",
    "if summary['total_reviews']%parameters['num_per_page']!=0:\n",
    "    parameters['num_per_page']=(summary['total_reviews']%parameters['num_per_page'])\n",
    "    response= requests.get( url= URL, params=parameters)\n",
    "    raw_data=response.json()\n",
    "    raw_reviews=raw_data['reviews']\n",
    "    review_data=review_data.append(pd.json_normalize(raw_reviews, max_level = 2))\n",
    "\n",
    "review_data.drop_duplicates(keep='first', inplace=True)\n",
    "review_data.reset_index(inplace=True)\n",
    "review_data.to_json(path_or_buf= 'SteamData/SteamReviews.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##reload review data set and get summary\n",
    "review_data=pd.read_json(path_or_buf= 'SteamData/SteamReviews.json', orient='columns')\n",
    "with open('SteamData/SteamReviewSummary.txt', 'r') as file:\n",
    "    review_summary= ast.literal_eval(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text=review_data['review'].copy()\n",
    "review_rating= review_data['voted_up'].copy()\n",
    "def decontracted(phrase):\n",
    "    \n",
    "    # specific\n",
    "    phrase = re.sub(r'tl;dr', 'tldr', phrase)\n",
    "    phrase = re.sub(r'won\\'t', 'will not', phrase)\n",
    "    phrase = re.sub(r'can\\'t', 'can not', phrase)\n",
    "    phrase = re.sub(r'\\bdont\\b', 'do not', phrase)\n",
    "    phrase = re.sub(r'\\bwont\\b', 'will not', phrase)\n",
    "    phrase = re.sub(r'\\bcant\\b', 'can not', phrase)\n",
    "    phrase = re.sub(r'\\bcannot\\b', 'can not', phrase)\n",
    "    phrase = re.sub(r'let\\'s', 'let us', phrase)\n",
    "    phrase = re.sub(r'w/', 'with', phrase)\n",
    "    phrase = re.sub(r'w/o', 'without', phrase)\n",
    "    phrase = re.sub(r'\\bive\\b', 'i have', phrase)\n",
    "    phrase = re.sub(r'\\blets\\b', 'let us', phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r'n\\'t', ' not', phrase)\n",
    "    phrase = re.sub(r'\\'re', ' are', phrase)\n",
    "    phrase = re.sub(r'\\'s', ' is', phrase)\n",
    "    phrase = re.sub(r'\\'d', ' would', phrase)\n",
    "    phrase = re.sub(r'\\'ll', ' will', phrase)\n",
    "    phrase = re.sub(r'\\'t', ' not', phrase)\n",
    "    phrase = re.sub(r'\\'ve', ' have', phrase)\n",
    "    phrase = re.sub(r'\\'m', ' am', phrase)\n",
    "    return phrase\n",
    "\n",
    "#clean data: removing symbols/formatting characters, making all characters lower case, removing contractions\n",
    "\n",
    "for i in range(len(review_text)): \n",
    "    review_text[i]=decontracted(review_text[i].lower())\n",
    "    review_text[i]=re.sub(r'(http\\S+|\\[.*?\\])|\\\\n|\\W|(\\$\\S+)|\\d|[^\\x00-\\x7A]', ' ', review_text[i])\n",
    "    \n",
    "#convert boolean strings to int for reviews (1 means a positive review, 0 means negative)\n",
    "rating_encoded=[]\n",
    "for i in review_rating:\n",
    "    rating_encoded.append(int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3748, 21) {'num_reviews': 1, 'review_score': 5, 'review_score_desc': 'Mixed', 'total_positive': 3966, 'total_negative': 5299, 'total_reviews': 9265}\n"
     ]
    }
   ],
   "source": [
    "print(review_data.shape, review_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8010"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting the reviews to word vectors\n",
    "\n",
    "#initialize a counter object\n",
    "word_count =Counter()\n",
    "\n",
    "#For each review in our file, we update our counter with any new words and increase the count of words already seen.\n",
    "for review in review_text:\n",
    "    word_count.update(review.split())\n",
    "    \n",
    "#identifying words only used once, removing most common words (like the, and, a)\n",
    "top_word_list = word_count.most_common(5)\n",
    "one_appearance_word_list = [w for w in word_count if word_count[w] == 1]\n",
    "#len(one_appearance_word_list)\n",
    "#len(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing useless words, \n",
    "\n",
    "#initializing a regex pattern for removing words\n",
    "one_word_regex=''\n",
    "for word in one_appearance_word_list:\n",
    "    one_word_regex+='\\\\b'+ word + '\\\\b|'\n",
    "    \n",
    "for word, count in top_word_list:  \n",
    "    one_word_regex+='\\\\b'+ word + '\\\\b|'\n",
    "pattern=re.compile(one_word_regex.rstrip('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text_trimmed=[]\n",
    "#removing words that appear one time, or appear so often that they contain no information\n",
    "for i in range(len(review_text)):\n",
    "    review_text_trimmed.append(re.sub(pattern,'', review_text[i]))\n",
    "    review_text_trimmed[i]=re.sub('\\s{2,}',' ', review_text_trimmed[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "honestly i have disagree with most negative reviews this game is great it is not exactly dow it is very different from but on its own merits it is solid warhammer game for one thing it is an rts again with base building proper armies which is welcome return form all three different armies have their particular playstyle they have done them justice as space marines drop dreadnought in middle fight or crush enemy infantry under your drop pod while you are at it sweet as eldar teleport your entire base away strike beautiful as orks waaagh it certainly feels like proper warhammer k this is helped by fantastic score by paul morgan recent solid voice acting let us face it really sucked in that department singleplayer campaign is worth entry price alone it has great story with some proper k moments that make you go fuck yeah as far as i am concerned last two missions this game are new definition epic give it chance you will not regret it proper k games are not dime dozen this is good one looking forward next campaign they teased emperor wants you leave positive review  \n",
      " honestly  i have to disagree with most of the negative reviews   this game is great  it is not exactly dow   and it is very different from    but on its own merits  it is a solid warhammer game   for one thing  it is an rts again  with base building and proper armies  which is a welcome return to form  and all three of the different armies have their particular playstyle  and they have done them justice  as space marines  drop a dreadnought in the middle of a fight   and scatter or crush the enemy infantry under your drop pod while you are at it   sweet  as eldar  teleport your entire base away and strike unseen   beautiful  as the orks    waaagh     it certainly feels like proper warhammer   k  and this is helped by the fantastic score  by paul leonard morgan of recent dredd fame   and the solid voice acting  let us face it        really sucked in that department    the singleplayer campaign is worth the entry price alone  it has a great story  with some proper   k moments that make you go  fuck yeah   as far as i am concerned  the last two missions of this game are the new definition of epic   give it a chance  you will not regret it  proper   k games are not a dime a dozen  and this is a good one  looking forward to the next campaign they teased   the emperor wants you to leave a positive review \n"
     ]
    }
   ],
   "source": [
    "print(review_text_trimmed[3724], '\\n', review_text[3724])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and save encoder for our reviews,\n",
    "encoder = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    review_text_trimmed, target_vocab_size=len(word_count))\n",
    "encoder.save_to_file('SteamData/SteamReviewVocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder= encoder.load_from_file('SteamData/SteamReviewVocab')\n",
    "\n",
    "#encode the words \n",
    "encoded_reviews=[]\n",
    "for i in review_text_trimmed:\n",
    "    encoded_reviews.append(encoder.encode(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating tensorflow datasets for training\n",
    "def labeler(review, rating):\n",
    "    return review, rating\n",
    "#pairing the labels (good/bad game) with the encoded reviews\n",
    "encoded_review_rating_list=[]\n",
    "for i,j in enumerate(encoded_reviews):\n",
    "    encoded_review_dataset = tf.data.Dataset.from_tensors(j)\n",
    "    encoded_review_rating_list.append(encoded_review_dataset.map(lambda x: labeler(x,rating_encoded[i])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the list of review:score sets into a single tensor dataset.\n",
    "encoded_review_ratings = encoded_review_rating_list[0]\n",
    "for labeled_dataset in encoded_review_rating_list[1:]:\n",
    "    encoded_review_ratings=encoded_review_ratings.concatenate(labeled_dataset)\n",
    "\n",
    "#Shuffle the datasets to avoid any biases.\n",
    "buffer_size = len(encoded_reviews)\n",
    "all_labeled_data = encoded_review_ratings.shuffle(\n",
    "    buffer_size, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Split the encoded words into training and test datasets, take size amount of data that goes into the training set\n",
    "training_ratio=0.6\n",
    "take_size= round(len(encoded_reviews)*training_ratio)\n",
    "batch_size=20\n",
    "\n",
    "train_data = encoded_review_ratings.take(take_size).shuffle(buffer_size)\n",
    "train_data = train_data.padded_batch(batch_size, padded_shapes=([None],()))\n",
    "\n",
    "test_data = encoded_review_ratings.skip(take_size)\n",
    "test_data = test_data.padded_batch(batch_size, padded_shapes=([None],()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 16)          155728    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               1700      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 162,529\n",
      "Trainable params: 162,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(encoder.vocab_size, embedding_dim),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    layers.Dense(50, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=10,\n",
    "    validation_data=test_data, validation_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-b9dd9b019e0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext_dummy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_dummy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtext_dummy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_dummy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    416\u001b[0m     if (context.executing_eagerly()\n\u001b[0;32m    417\u001b[0m         or ops.get_default_graph()._building_function):  # pylint: disable=protected-access\n\u001b[1;32m--> 418\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    592\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[0;32m    593\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/cpu:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    598\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[1;31m# Store dataset reference to ensure that dataset is alive when this iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_apply_options\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m     \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_threading\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m       \u001b[0mt_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_threading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36moptions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0minput_dataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m       \u001b[0minput_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0minput_options\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "... last 1 frames repeated, from the frame below ...\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36moptions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0minput_dataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m       \u001b[0minput_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0minput_options\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "text_dummy, label_dummy = next(iter(test_data))\n",
    "\n",
    "text_dummy[0], label_dummy[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
