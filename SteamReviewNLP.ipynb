{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Getting reviews dataset and saving it.\n",
    "URL= 'https://store.steampowered.com/appreviews/285190'\n",
    "cursor_string='*'\n",
    "cursor_list=['']\n",
    "review_data= pd.DataFrame()\n",
    "parameters={'json': 1,'filter': 'all','language': 'english','day_range': 5000,'cursor': cursor_string,\n",
    "            'review_type': 'all','purchase_type': 'all','num_per_page': 1}\n",
    "#we access the reviews the first time in order to ascertain how many reviews there are to download\n",
    "response= requests.get( url= URL, params=parameters)\n",
    "raw_data=response.json()\n",
    "summary=raw_data['query_summary']\n",
    "\n",
    "#save the summary as a txt file\n",
    "summaryString= json.dumps(summary)\n",
    "with open('C:/Users/Malcolm/Untitled Folder/SteamData/SteamReviewSummary.txt', 'w') as file:\n",
    "    file.write(summaryString)\n",
    "    \n",
    "#set the maximum number of reviews per request\n",
    "parameters['num_per_page']=100\n",
    "#calculate how many times to loop the get request, based on the total number of reviews in our query summary\n",
    "request_iterations =1+int(summary['total_reviews']/parameters['num_per_page'])\n",
    "\n",
    "for i in range(request_iterations):\n",
    "    response= requests.get( url= URL, params=parameters)\n",
    "    raw_data=response.json()\n",
    "    raw_reviews=raw_data['reviews']\n",
    "    review_data=review_data.append(pd.json_normalize(raw_reviews, max_level = 2))\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    cursor_list+=cursor_string\n",
    "    cursor_string=raw_data['cursor']\n",
    "    parameters['cursor']= cursor_string\n",
    "    \n",
    "    #get the last remaining set of reviews (not included in the 100 per page iterations)\n",
    "if summary['total_reviews']%parameters['num_per_page']!=0:\n",
    "    parameters['num_per_page']=(summary['total_reviews']%parameters['num_per_page'])\n",
    "    response= requests.get( url= URL, params=parameters)\n",
    "    raw_data=response.json()\n",
    "    raw_reviews=raw_data['reviews']\n",
    "    review_data=review_data.append(pd.json_normalize(raw_reviews, max_level = 2))\n",
    "\n",
    "review_data.drop_duplicates(keep='first', inplace=True)\n",
    "review_data.reset_index(inplace=True)\n",
    "review_data.to_json(path_or_buf= 'C:/Users/Malcolm/Untitled Folder/SteamData/SteamReviews.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##reload review data set and get summary\n",
    "review_data=pd.read_json(path_or_buf= 'C:/Users/Malcolm/Untitled Folder/SteamData/SteamReviews.json', orient='columns')\n",
    "with open('C:/Users/Malcolm/Untitled Folder/SteamData/SteamReviewSummary.txt', 'r') as file:\n",
    "    review_summary= ast.literal_eval(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text=review_data['review'].copy()\n",
    "review_rating= review_data['voted_up'].copy()\n",
    "def decontracted(phrase):\n",
    "    \n",
    "    # specific\n",
    "    phrase = re.sub(r'tl;dr', 'tldr', phrase)\n",
    "    phrase = re.sub(r'won\\'t', 'will not', phrase)\n",
    "    phrase = re.sub(r'can\\'t', 'can not', phrase)\n",
    "    phrase = re.sub(r'\\bdont\\b', 'do not', phrase)\n",
    "    phrase = re.sub(r'\\bwont\\b', 'will not', phrase)\n",
    "    phrase = re.sub(r'\\bcant\\b', 'can not', phrase)\n",
    "    phrase = re.sub(r'\\bcannot\\b', 'can not', phrase)\n",
    "    phrase = re.sub(r'let\\'s', 'let us', phrase)\n",
    "    phrase = re.sub(r'w/', 'with', phrase)\n",
    "    phrase = re.sub(r'w/o', 'without', phrase)\n",
    "    phrase = re.sub(r'\\bive\\b', 'i have', phrase)\n",
    "    phrase = re.sub(r'\\blets\\b', 'let us', phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r'n\\'t', ' not', phrase)\n",
    "    phrase = re.sub(r'\\'re', ' are', phrase)\n",
    "    phrase = re.sub(r'\\'s', ' is', phrase)\n",
    "    phrase = re.sub(r'\\'d', ' would', phrase)\n",
    "    phrase = re.sub(r'\\'ll', ' will', phrase)\n",
    "    phrase = re.sub(r'\\'t', ' not', phrase)\n",
    "    phrase = re.sub(r'\\'ve', ' have', phrase)\n",
    "    phrase = re.sub(r'\\'m', ' am', phrase)\n",
    "    return phrase\n",
    "\n",
    "#clean data: removing symbols/formatting characters, making all characters lower case, removing contractions\n",
    "\n",
    "for i in range(len(review_text)): \n",
    "    review_text[i]=decontracted(review_text[i].lower())\n",
    "    review_text[i]=re.sub(r'(http\\S+|\\[.*?\\])|\\\\n|\\W|(\\$\\S+)|\\d|[^\\x00-\\x7A]', ' ', review_text[i])\n",
    "    \n",
    "#convert boolean strings to int for reviews (1 means a positive review, 0 means negative)\n",
    "rating_encoded=[]\n",
    "for i in review_rating:\n",
    "    rating_encoded.append(int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3750, 21) {'num_reviews': 1, 'review_score': 5, 'review_score_desc': 'Mixed', 'total_positive': 3966, 'total_negative': 5299, 'total_reviews': 9265}\n"
     ]
    }
   ],
   "source": [
    "print(review_data.shape, review_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7992"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting the reviews to word vectors\n",
    "\n",
    "#initialize a counter object\n",
    "word_count =Counter()\n",
    "\n",
    "#For each review in our file, we update our counter with any new words and increase the count of words already seen.\n",
    "for review in review_text:\n",
    "    word_count.update(review.split())\n",
    "    \n",
    "#identifying words only used once, removing most common words (like the, and, a)\n",
    "top_word_list = word_count.most_common(5)\n",
    "one_appearance_word_list = [w for w in word_count if word_count[w] == 1]\n",
    "len(one_appearance_word_list)\n",
    "#len(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing useless words, \n",
    "\n",
    "#initializing a regex pattern for removing words\n",
    "one_word_regex=''\n",
    "for word in one_appearance_word_list:\n",
    "    one_word_regex+='\\\\b'+ word + '\\\\b|'\n",
    "    \n",
    "for word, count in top_word_list:  \n",
    "    one_word_regex+='\\\\b'+ word + '\\\\b|'\n",
    "pattern=re.compile(one_word_regex.rstrip('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text_trimmed=[]\n",
    "#removing words that appear one time, or appear so often that they contain no information\n",
    "for i in range(len(review_text)):\n",
    "    review_text_trimmed.append(re.sub(pattern,'', review_text[i]))\n",
    "    review_text_trimmed[i]=re.sub('\\s{2,}',' ', review_text_trimmed[i])\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "honestly i have disagree with most negative reviews this game is great it is not exactly dow it is very different from but on its own merits it is solid warhammer game for one thing it is an rts again with base building proper armies which is welcome return form all three different armies have their particular playstyle they have done them justice as space marines drop dreadnought in middle fight or crush enemy infantry under your drop pod while you are at it sweet as eldar teleport your entire base away strike beautiful as orks waaagh it certainly feels like proper warhammer k this is helped by fantastic score by paul morgan recent solid voice acting let us face it really sucked in that department singleplayer campaign is worth entry price alone it has great story with some proper k moments that make you go fuck yeah as far as i am concerned last two missions this game are new definition epic give it chance you will not regret it proper k games are not dime dozen this is good one looking forward next campaign they teased emperor wants you leave positive review  \n",
      " honestly  i have to disagree with most of the negative reviews   this game is great  it is not exactly dow   and it is very different from    but on its own merits  it is a solid warhammer game   for one thing  it is an rts again  with base building and proper armies  which is a welcome return to form  and all three of the different armies have their particular playstyle  and they have done them justice  as space marines  drop a dreadnought in the middle of a fight   and scatter or crush the enemy infantry under your drop pod while you are at it   sweet  as eldar  teleport your entire base away and strike unseen   beautiful  as the orks    waaagh     it certainly feels like proper warhammer   k  and this is helped by the fantastic score  by paul leonard morgan of recent dredd fame   and the solid voice acting  let us face it        really sucked in that department    the singleplayer campaign is worth the entry price alone  it has a great story  with some proper   k moments that make you go  fuck yeah   as far as i am concerned  the last two missions of this game are the new definition of epic   give it a chance  you will not regret it  proper   k games are not a dime a dozen  and this is a good one  looking forward to the next campaign they teased   the emperor wants you to leave a positive review \n"
     ]
    }
   ],
   "source": [
    "print(review_text_trimmed[3724], '\\n', review_text[3724])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Failed to create a NewWriteableFile: C:/Users/Malcolm/Untitled Folder/SteamData/SteamReviewVocab.subwords : Das System kann den angegebenen Pfad nicht finden.\r\n; No such process",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-717b88e78f24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m encoder = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n\u001b[0;32m      3\u001b[0m     review_text_trimmed, target_vocab_size=len(word_count))\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/Malcolm/Untitled Folder/SteamData/SteamReviewVocab'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_datasets\\core\\features\\text\\subword_text_encoder.py\u001b[0m in \u001b[0;36msave_to_file\u001b[1;34m(self, filename_prefix)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"'%s'\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_subwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_write_lines_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_datasets\\core\\features\\text\\text_encoder.py\u001b[0m in \u001b[0;36m_write_lines_to_file\u001b[1;34m(cls, filename, lines, metadata_dict)\u001b[0m\n\u001b[0;32m    109\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_write_lines_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;34m\"\"\"Writes lines to file prepended by header and metadata.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m     \u001b[0mwrite_lines_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_datasets\\core\\features\\text\\text_encoder.py\u001b[0m in \u001b[0;36mwrite_lines_to_file\u001b[1;34m(cls_name, filename, lines, metadata_dict)\u001b[0m\n\u001b[0;32m    509\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mheader_line\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata_line\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m       \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    512\u001b[0m       \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, file_content)\u001b[0m\n\u001b[0;32m    104\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;34m\"\"\"Writes file_content to the file. Appends to the end of the file.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prewrite_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     pywrap_tensorflow.AppendToFile(\n\u001b[0;32m    108\u001b[0m         compat.as_bytes(file_content), self._writable_file)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36m_prewrite_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     90\u001b[0m                                            \"File isn't open for writing\")\n\u001b[0;32m     91\u001b[0m       self._writable_file = pywrap_tensorflow.CreateWritableFile(\n\u001b[1;32m---> 92\u001b[1;33m           compat.as_bytes(self.__name), compat.as_bytes(self.__mode))\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_prepare_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Failed to create a NewWriteableFile: C:/Users/Malcolm/Untitled Folder/SteamData/SteamReviewVocab.subwords : Das System kann den angegebenen Pfad nicht finden.\r\n; No such process"
     ]
    }
   ],
   "source": [
    "#create and save encoder for our reviews,\n",
    "encoder = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    review_text_trimmed, target_vocab_size=len(word_count))\n",
    "encoder.save_to_file('C:/Users/Malcolm/Untitled Folder/SteamData/SteamReviewVocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder= encoder.load_from_file('C:/Users/Malcolm/Untitled Folder/SteamData/SteamReviewVocab')\n",
    "\n",
    "#encode the words \n",
    "encoded_reviews=[]\n",
    "for i in review_text_trimmed:\n",
    "    encoded_reviews.append(encoder.encode(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating tensorflow datasets for training\n",
    "def labeler(review, rating):\n",
    "    return review, rating\n",
    "\n",
    "encoded_review_rating_list=[]\n",
    "for i,j in enumerate(encoded_reviews):\n",
    "    encoded_review_dataset = tf.data.Dataset.from_tensors(j)\n",
    "    encoded_review_rating_list.append(encoded_review_dataset.map(lambda x: labeler(x,rating_encoded[i])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine these labeled datasets into a single dataset, and shuffle it.\n",
    "encoded_review_ratings = encoded_review_rating_list[0]\n",
    "for labeled_dataset in encoded_review_rating_list[1:]:\n",
    "    encoded_review_ratings=encoded_review_ratings.concatenate(labeled_dataset)\n",
    "    \n",
    "buffer_size = len(encoded_reviews)\n",
    "all_labeled_data = encoded_review_ratings.shuffle(\n",
    "    buffer_size, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Split the encoded words into training and test datasets, take size is the fraction that goes into the training set\n",
    "training_ratio=0.6\n",
    "take_size= round(len(encoded_reviews)*training_ratio)\n",
    "batch_size=10\n",
    "\n",
    "train_data = encoded_review_ratings.take(take_size).shuffle(buffer_size)\n",
    "train_data = train_data.padded_batch(batch_size, padded_shapes=([None],()))\n",
    "\n",
    "test_data = encoded_review_ratings.skip(take_size)\n",
    "test_data = test_data.padded_batch(batch_size, padded_shapes=([None],()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "model = keras.Sequential([\n",
    "  layers.Embedding(encoder.vocab_size, embedding_dim),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dense(16, activation='relu'),\n",
    "  layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 225 steps, validate for 5 steps\n",
      "Epoch 1/10\n",
      "\r",
      "  0/225 [..............................] - ETA: 0s"
     ]
    },
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-7b8c616c92c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     validation_data=test_data, validation_steps=5)\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m                 \u001b[0mtraining_data_iter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initializer\u001b[0m  \u001b[1;31m# pylint: disable=pointless-statement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m               \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m                 \u001b[0mtraining_data_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m             training_result = run_one_epoch(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    416\u001b[0m     if (context.executing_eagerly()\n\u001b[0;32m    417\u001b[0m         or ops.get_default_graph()._building_function):  # pylint: disable=protected-access\n\u001b[1;32m--> 418\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    592\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[0;32m    593\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/cpu:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    598\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[1;31m# Store dataset reference to ensure that dataset is alive when this iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_apply_options\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m     \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_threading\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m       \u001b[0mt_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_threading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36moptions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0minput_dataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m       \u001b[0minput_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0minput_options\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "... last 1 frames repeated, from the frame below ...\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36moptions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0minput_dataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m       \u001b[0minput_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0minput_options\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=10,\n",
    "    validation_data=test_data, validation_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorDataset shapes: (80,), types: tf.int32>"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#labeler(encoded_reviews[1],1)\n",
    "#tf.data.Dataset.from_tensors(encoded_reviews[1])\n",
    "#print(encoded_review_rating_list)\n",
    "#tf.data.Dataset.from_tensors(rating_encoded[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<MapDataset shapes: ((80,), ()), types: (tf.int32, tf.int32)>,\n",
       " <MapDataset shapes: ((850,), ()), types: (tf.int32, tf.int32)>,\n",
       " <MapDataset shapes: ((105,), ()), types: (tf.int32, tf.int32)>,\n",
       " <MapDataset shapes: ((594,), ()), types: (tf.int32, tf.int32)>]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_review_rating_list[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10674"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConcatenateDataset shapes: ((None,), ()), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "print(encoded_review_ratings)#.concatenate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([10450,     4,    20,   466,    73,   344,     1,     3,   173,\n",
      "           3,    36,    47,   486,   739,    33,    51,  5849,   275,\n",
      "          99,  1455,   614,     9,   202,   443,    18,    40,     5,\n",
      "        1440,  1455,   614,    67,     4,  2801,    12,     4,  2801,\n",
      "          67,    18,    79,     5,  1440,  1455,   614,    18,   298,\n",
      "          18,    99,   188,   359,    19,    75,   367,    45,    10,\n",
      "          60,     2,    71,   188,   359,    19,    75,  1524,     9,\n",
      "          23,     5,  1468,  1215,    38]), 0)\n",
      "(array([ 239,    7,    4,    1,   59,  726,   12,  424,  400,  312,    6,\n",
      "        145,  202,  758,  229,   19,    3,    1,  325,    9,    3,  101,\n",
      "         15,  542,   81,  743,  112,   83,  120,    4,   12,  197,  253,\n",
      "        965,   27, 2135,  589, 4687,   91,  614,  279,   27, 3504,   91,\n",
      "        342,  111,    4, 1280,   88,  177,   46,  107, 3134, 4234, 1408,\n",
      "          2,   15,  228,  425,    9,  825, 2293,   42, 4347, 2779,    8,\n",
      "        663,  643, 2758, 3038,   23,    5,  756,   91, 3638,  694,  229,\n",
      "         10,    7,    4]), 0)\n"
     ]
    }
   ],
   "source": [
    "dummy=encoded_review_rating_list[0].concatenate(encoded_review_rating_list[1])\n",
    "\n",
    "for e in dummy.as_numpy_iterator():\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
