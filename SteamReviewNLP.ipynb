{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "#import sys\n",
    "#sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the encoder, encode the reviews, and positive/negative review list\n",
    "encoder=tfds.features.text.SubwordTextEncoder.load_from_file('SteamData/SteamReviewVocab')\n",
    "\n",
    "    \n",
    "generic = lambda x: ast.literal_eval(x)\n",
    "\n",
    "starting_dataset=pd.read_csv('SteamData/FormattedReviewRatingList.csv', converters={'Rating': generic})\n",
    "trimmed_review=starting_dataset.loc[:, 'Trimmed Review'].to_numpy()\n",
    "ratings=starting_dataset.drop('Trimmed Review', axis=1).to_numpy(dtype='int32')\n",
    "\n",
    "\n",
    "total_dataset=tf.data.experimental.make_csv_dataset(\n",
    "    'SteamData/FormattedReviewRatingList.csv', batch_size=1, column_names=['Trimmed Review', 'Rating'], column_defaults=['string', 'int32'], \n",
    "    label_name='Rating', field_delim=',', use_quote_delim=True, header=True, num_epochs=None, shuffle=True,\n",
    "    shuffle_buffer_size=len(ratings)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the text\n",
    "#result = total_dataset.map(lambda review_str, rating_int: encoder.encode(review_str))\n",
    "\n",
    "encoded_reviews=[]\n",
    "for i, j in enumerate(trimmed_review):\n",
    "    encoded_reviews.append(encoder.encode(j))\n",
    "\n",
    "#creating tensorflow datasets for training\n",
    "def labeler(review, rating):\n",
    "    return review, rating\n",
    "#pairing the labels (good/bad game) with the encoded reviews\n",
    "encoded_review_rating_list=[]\n",
    "for i,j in enumerate(encoded_reviews):\n",
    "    encoded_review_dataset = tf.data.Dataset.from_tensors(tf.cast(j, dtype='int64'))\n",
    "    encoded_review_rating_list.append(encoded_review_dataset.map(lambda x: labeler(x,ratings[i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Combine the list of review:score sets into a single tensor dataset.\n",
    "encoded_review_ratings = encoded_review_rating_list[0]\n",
    "#test_var_tensor=tf.constant()\n",
    "for single_dataset in encoded_review_rating_list[1:]:\n",
    "    encoded_review_ratings=encoded_review_ratings.concatenate(single_dataset)\n",
    "\n",
    "#Shuffle the datasets to avoid any biases.\n",
    "buffer_size = len(encoded_reviews)\n",
    "all_labeled_data = encoded_review_ratings.shuffle(\n",
    "    buffer_size, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Split the encoded words into training and test datasets, take size amount of data that goes into the training set\n",
    "training_ratio=0.6\n",
    "take_size= round(len(encoded_reviews)*training_ratio)\n",
    "batch_size=30\n",
    "\n",
    "#Organizing our training and validation data, the padded shapes are set to the longest review (as specified by None keywords)\n",
    "train_data = encoded_review_ratings.take(take_size)\n",
    "train_data = train_data.padded_batch(batch_size, padded_shapes=((None,), (1,)))\n",
    "\n",
    "test_data = encoded_review_ratings.skip(take_size)\n",
    "test_data = test_data.padded_batch(batch_size, padded_shapes=((None,), (1,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 10)          33080     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 200)               2200      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 60,481\n",
      "Trainable params: 60,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim=10\n",
    "learn_rate= 0.001\n",
    "#determining layers of our neural network\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(encoder.vocab_size, embedding_dim),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(200, activation='relu'),\n",
    "    layers.Dropout(rate=0.15),\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    layers.Dense(50, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the optimizer and loss equation here, then compile and run the model.\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learn_rate)\n",
    "loss_eqn=tf.keras.losses.BinaryCrossentropy(from_logits='True')\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss=loss_eqn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=30,\n",
    "    validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the accuracy and losses for our training and validation sets.\n",
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss=history_dict['loss']\n",
    "val_loss=history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim((0.5,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract TSV files of the word embeddings, to be used with the embedding project http://projector.tensorflow.org/\n",
    "import io\n",
    "\n",
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for num, word in enumerate(encoder.subwords):\n",
    "    vec = weights[num+1] # skip 0, it's padding.\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in encoded_review_ratings.take(1):\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConcatenateDataset shapes: ((None,), (1,)), types: (tf.int64, tf.int32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_review_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  67  381   31 ...    0    0    0]\n",
      " [3084   27    2 ...    0    0    0]\n",
      " [   3  113    9 ...    1    9  103]\n",
      " ...\n",
      " [ 101    1   38 ...    0    0    0]\n",
      " [   9    7    5 ...    0    0    0]\n",
      " [ 572    1   40 ...    0    0    0]], shape=(30, 452), dtype=int64) tf.Tensor(\n",
      "[[1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]], shape=(30, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "next_feature, next_label = next(iter(test_data))\n",
    "\n",
    "print (next_feature, next_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
