{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "#import sys\n",
    "#sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the encoder and trimmed review file\n",
    "encoder=tfds.features.text.SubwordTextEncoder.load_from_file('SteamData/SteamReviewVocab')\n",
    "\n",
    "with open('SteamData/SteamReviewTrimmed.txt', 'r') as file:\n",
    "        review_text_trimmed=file.readlines()\n",
    "\n",
    "#remove newline characters\n",
    "for j,item in enumerate(review_text_trimmed):\n",
    "    review_text_trimmed[j]=re.sub('\\n','', item)\n",
    "    \n",
    "#encode the words \n",
    "encoded_reviews=[]\n",
    "for i in review_text_trimmed:\n",
    "    encoded_reviews.append(encoder.encode(i))\n",
    "\n",
    "#convert boolean strings to int for reviews (1 means a positive review, 0 means negative)\n",
    "review_data=pd.read_json(path_or_buf= 'SteamData/SteamReviews.json', orient='columns')\n",
    "review_rating= review_data['voted_up'].copy()\n",
    "rating_encoded=[]\n",
    "for i in review_rating:\n",
    "    rating_encoded.append(int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating tensorflow datasets for training\n",
    "def labeler(review, rating):\n",
    "    return review, rating\n",
    "#pairing the labels (good/bad game) with the encoded reviews\n",
    "encoded_review_rating_list=[]\n",
    "for i,j in enumerate(encoded_reviews):\n",
    "    encoded_review_dataset = tf.data.Dataset.from_tensors(j)\n",
    "    encoded_review_rating_list.append(encoded_review_dataset.map(lambda x: labeler(x,rating_encoded[i])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the list of review:score sets into a single tensor dataset.\n",
    "encoded_review_ratings = encoded_review_rating_list[0]\n",
    "#test_var_tensor=tf.constant()\n",
    "for single_dataset in encoded_review_rating_list[1:]:\n",
    "    encoded_review_ratings=encoded_review_ratings.concatenate(single_dataset)\n",
    "\n",
    "encoded_review_ratings=encoded_review_ratings\n",
    "#Shuffle the datasets to avoid any biases.\n",
    "buffer_size = len(encoded_reviews)\n",
    "all_labeled_data = encoded_review_ratings.shuffle(\n",
    "    buffer_size, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Split the encoded words into training and test datasets, take size amount of data that goes into the training set\n",
    "training_ratio=0.9\n",
    "take_size= round(len(encoded_reviews)*training_ratio)\n",
    "batch_size=30\n",
    "\n",
    "#Organizing our training and validation data, the padded shapes are set to the longest review (as specified by None keywords)\n",
    "train_data = all_labeled_data.take(take_size)\n",
    "train_data = train_data.padded_batch(batch_size, padded_shapes=([None],()))\n",
    "\n",
    "test_data = all_labeled_data.skip(take_size)\n",
    "test_data = test_data.padded_batch(batch_size, padded_shapes=([None],()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 10)          213580    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 200)               2200      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 240,981\n",
      "Trainable params: 240,981\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim=10\n",
    "learn_rate= 0.001\n",
    "#determining layers of our neural network\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(encoder.vocab_size, embedding_dim),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(200, activation='relu'),\n",
    "    layers.Dropout(rate=0.15),\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    layers.Dense(50, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the optimizer and loss equation here, then compile and run the model.\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learn_rate)\n",
    "loss_eqn=tf.keras.losses.BinaryCrossentropy(from_logits='True')\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss=loss_eqn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=30,\n",
    "    validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the accuracy and losses for our training and validation sets.\n",
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss=history_dict['loss']\n",
    "val_loss=history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim((0.5,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract TSV files of the word embeddings, to be used with the embedding project http://projector.tensorflow.org/\n",
    "import io\n",
    "\n",
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for num, word in enumerate(encoder.subwords):\n",
    "    vec = weights[num+1] # skip 0, it's padding.\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(122,), dtype=int32, numpy=\n",
      "array([1278, 1990,    6,    4,  938,    9,   35,  693,    2,    5,   41,\n",
      "          2,  398,  315,  106,  505,   13,  237,   79,   59,  535,   60,\n",
      "         20,  997,  314, 1918,  133,  129,  217,  130, 1492,   17,  122,\n",
      "         18,    6,    4,  100, 1720, 1207,  236,    1,    5,   24, 1913,\n",
      "          1,   87,   43,  164,  102,    7,  489,   13,  126,  463,  105,\n",
      "       2435, 1757,  463,  105, 1099,  532, 1099,  159,  236,    1,   24,\n",
      "       1954,    5,  235,    4,  346,  315,  169, 1981,   87,   50,  443,\n",
      "        129,    4,   16,    9,   13, 2579,   73, 1290,    7,    1,   12,\n",
      "        294,   10,  436,  752,  800,   19,  491,   17,   32,  162,  556,\n",
      "          2,   87,  321,    4,   16,    6, 1061,  182,  126,    3,   12,\n",
      "         72,  311,  131,    4,    7,   12,  100,   36,   17,    6, 1535,\n",
      "          7])>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n",
      "(<tf.Tensor: shape=(114,), dtype=int32, numpy=\n",
      "array([   4,   27,   20,   89,   20,    2, 1083,   26,    7,  187,    6,\n",
      "          4,   57,  680,   52,    1,  259,   39,  119,    3,   31,   24,\n",
      "         41,    4,    1,   33,   14,  257,    2, 1013,    8,  560,  525,\n",
      "          1, 1032,    6, 1368,   25,  447,   76,   14,  273, 2215,   30,\n",
      "        180,    2,   45,   87,  584,    4,   26,    7,   15,  376,   10,\n",
      "         53,  213,   13,   23,    1,   33,   14,  774,    2,  201,  375,\n",
      "        839,   27,   20,   89,   20,  606,   29,   67,  225,   11,   13,\n",
      "         75,   85,   39,  130,   15,  252,  884,   25,  192,   10, 1439,\n",
      "       1258,  266,   24,  252, 1092,    1, 1597,   70,    3,  140,    6,\n",
      "          4,   23,    1,   33,    2,   14,  774,  520,    2,   11,    5,\n",
      "          9,   39,  269,  191])>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: shape=(40,), dtype=int32, numpy=\n",
      "array([ 514,    3,   16,   13,   21,    5,  259,  249,    2,   90,   40,\n",
      "        293,   23,   19,  651,  271, 1131,   36,   13,   42,   12,  184,\n",
      "          4,   33,  100,  310,  148,   30,  506,   26,    7,   31,   46,\n",
      "          2,   17,   94,   39,   89,   97,  174])>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: shape=(22,), dtype=int32, numpy=\n",
      "array([  13,  400,   40,  269,  191,   63,   85,  804,    8,   13,   40,\n",
      "        208,   13, 2183,  493, 2269,   17,   94,   10,  208,   68,    1])>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n",
      "(<tf.Tensor: shape=(17,), dtype=int32, numpy=\n",
      "array([  9, 179, 488,  38,  48,  22,   3, 318,   6,   4,   1,  24,  85,\n",
      "        74,  71,  55, 262])>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "for ex in all_labeled_data.take(5):\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(30, 350), dtype=int32, numpy=\n",
      "array([[   3,   12,  104, ...,   33,   14,  317],\n",
      "       [  27,  256,    0, ...,    0,    0,    0],\n",
      "       [   3,   12,  147, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [   3,   45, 1980, ...,    0,    0,    0],\n",
      "       [2888,   98,   49, ...,    0,    0,    0],\n",
      "       [   6,    4,   18, ...,    0,    0,    0]])>, <tf.Tensor: shape=(30,), dtype=int32, numpy=\n",
      "array([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
      "       0, 0, 0, 1, 1, 1, 0, 0])>)\n",
      "(<tf.Tensor: shape=(30, 351), dtype=int32, numpy=\n",
      "array([[ 166,  134,   28, ...,    0,    0,    0],\n",
      "       [  50,  331,   80, ...,    0,    0,    0],\n",
      "       [ 849,  544, 1882, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [ 174,  102,   19, ...,    0,    0,    0],\n",
      "       [   7,   15,    5, ...,    0,    0,    0],\n",
      "       [  19, 1192,  144, ...,    0,    0,    0]])>, <tf.Tensor: shape=(30,), dtype=int32, numpy=\n",
      "array([0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
      "       0, 1, 1, 1, 0, 0, 0, 0])>)\n",
      "(<tf.Tensor: shape=(30, 371), dtype=int32, numpy=\n",
      "array([[ 641,   44,  163, ...,    0,    0,    0],\n",
      "       [ 289, 1075,   10, ...,    0,    0,    0],\n",
      "       [1075,    0,    0, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [  83,   71,   13, ...,    0,    0,    0],\n",
      "       [1462,   66,   61, ...,    0,    0,    0],\n",
      "       [ 200, 2433,    2, ...,    0,    0,    0]])>, <tf.Tensor: shape=(30,), dtype=int32, numpy=\n",
      "array([0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
      "       1, 0, 0, 1, 1, 0, 1, 1])>)\n",
      "(<tf.Tensor: shape=(14, 214), dtype=int32, numpy=\n",
      "array([[   2, 1383,   64, ...,    0,    0,    0],\n",
      "       [   6,   27,    4, ...,    0,    0,    0],\n",
      "       [  89,    6,    4, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [  48,   97,   16, ..., 1890,  675, 2400],\n",
      "       [ 152,    3,   33, ...,    0,    0,    0],\n",
      "       [ 101,  529,   47, ...,    0,    0,    0]])>, <tf.Tensor: shape=(14,), dtype=int32, numpy=array([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0])>)\n"
     ]
    }
   ],
   "source": [
    "for ex in test_data.take(5):\n",
    "  print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(30, 350), dtype=int32, numpy=\n",
       " array([[   3,   12,  104, ...,   33,   14,  317],\n",
       "        [  27,  256,    0, ...,    0,    0,    0],\n",
       "        [   3,   12,  147, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   3,   45, 1980, ...,    0,    0,    0],\n",
       "        [2888,   98,   49, ...,    0,    0,    0],\n",
       "        [   6,    4,   18, ...,    0,    0,    0]])>,\n",
       " <tf.Tensor: shape=(30,), dtype=int32, numpy=\n",
       " array([0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "        0, 0, 0, 1, 1, 1, 0, 0])>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text, sample_labels = next(iter(test_data))\n",
    "\n",
    "sample_text, sample_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f286209546cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msample_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_review_ratings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msample_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    416\u001b[0m     if (context.executing_eagerly()\n\u001b[0;32m    417\u001b[0m         or ops.get_default_graph()._building_function):  # pylint: disable=protected-access\n\u001b[1;32m--> 418\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    592\u001b[0m           context.context().device_spec.device_type != \"CPU\"):\n\u001b[0;32m    593\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/cpu:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    598\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[1;31m# Store dataset reference to ensure that dataset is alive when this iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_apply_options\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 358\u001b[1;33m     \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    359\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_threading\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m       \u001b[0mt_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_threading\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36moptions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0minput_dataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m       \u001b[0minput_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0minput_options\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "... last 1 frames repeated, from the frame below ...\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36moptions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0minput_dataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m       \u001b[0minput_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0minput_options\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "sample_text, sample_labels = next(iter(encoded_review_ratings))\n",
    "\n",
    "sample_text, sample_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConcatenateDataset shapes: ((None,), ()), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_review_ratings\n",
    "#<ConcatenateDataset shapes: ((None,), ()), types: (tf.int32, tf.int32)>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PaddedBatchDataset shapes: ((None, None), (None,)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data\n",
    "#<PaddedBatchDataset shapes: ((None, None), (None,)), types: (tf.int32, tf.int32)>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
