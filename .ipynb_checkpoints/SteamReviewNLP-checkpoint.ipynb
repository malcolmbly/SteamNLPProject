{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Getting reviews dataset and saving it.\n",
    "#The url is specific to each game/ app with a certain id\n",
    "URL= 'https://store.steampowered.com/appreviews/285190'\n",
    "cursor_string='*'\n",
    "cursor_list=['']\n",
    "review_data= pd.DataFrame()\n",
    "parameters={'json': 1,'filter': 'all','language': 'english','day_range': 5000,'cursor': cursor_string,\n",
    "            'review_type': 'all','purchase_type': 'all','num_per_page': 1}\n",
    "\n",
    "#we access the reviews the first time in order to ascertain how many reviews there are to download\n",
    "# this is a metric contained within query summary, however it seems to be innacurate\n",
    "response= requests.get( url= URL, params=parameters)\n",
    "raw_data=response.json()\n",
    "summary=raw_data['query_summary']\n",
    "\n",
    "#save the summary as a txt file\n",
    "summaryString= json.dumps(summary)\n",
    "with open('SteamData/SteamReviewSummary.txt', 'w') as file:\n",
    "    file.write(summaryString)\n",
    "    \n",
    "#set the maximum number of reviews per request\n",
    "parameters['num_per_page']=100\n",
    "#calculate how many times to loop the get request, based on the total number of reviews in our query summary\n",
    "request_iterations =1+int(summary['total_reviews']/parameters['num_per_page'])\n",
    "\n",
    "for i in range(request_iterations):\n",
    "    response= requests.get( url= URL, params=parameters)\n",
    "    raw_data=response.json()\n",
    "    raw_reviews=raw_data['reviews']\n",
    "    review_data=review_data.append(pd.json_normalize(raw_reviews, max_level = 2))\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    cursor_list+=cursor_string\n",
    "    cursor_string=raw_data['cursor']\n",
    "    parameters['cursor']= cursor_string\n",
    "    \n",
    "    #get the last remaining set of reviews (not included in the 100 per page iterations)\n",
    "if summary['total_reviews']%parameters['num_per_page']!=0:\n",
    "    parameters['num_per_page']=(summary['total_reviews']%parameters['num_per_page'])\n",
    "    response= requests.get( url= URL, params=parameters)\n",
    "    raw_data=response.json()\n",
    "    raw_reviews=raw_data['reviews']\n",
    "    review_data=review_data.append(pd.json_normalize(raw_reviews, max_level = 2))\n",
    "\n",
    "review_data.drop_duplicates(keep='first', inplace=True)\n",
    "review_data.reset_index(inplace=True)\n",
    "review_data.to_json(path_or_buf= 'SteamData/SteamReviews.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "##reload review data set and get summary\n",
    "review_data=pd.read_json(path_or_buf= 'SteamData/SteamReviews.json', orient='columns')\n",
    "with open('SteamData/SteamReviewSummary.txt', 'r') as file:\n",
    "    review_summary= ast.literal_eval(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text=review_data['review'].copy()\n",
    "review_rating= review_data['voted_up'].copy()\n",
    "\n",
    "#the decontracted method removed contractions and also a few common typos\n",
    "def decontracted(phrase):\n",
    "    \n",
    "    # specific\n",
    "    phrase = re.sub(r'tl;dr', 'tldr', phrase)\n",
    "    phrase = re.sub(r'won\\'t', 'will not', phrase)\n",
    "    phrase = re.sub(r'can\\'t', 'can not', phrase)\n",
    "    phrase = re.sub(r'\\bdont\\b', 'do not', phrase)\n",
    "    phrase = re.sub(r'\\bwont\\b', 'will not', phrase)\n",
    "    phrase = re.sub(r'\\bcant\\b', 'can not', phrase)\n",
    "    phrase = re.sub(r'\\bcannot\\b', 'can not', phrase)\n",
    "    phrase = re.sub(r'let\\'s', 'let us', phrase)\n",
    "    phrase = re.sub(r'w/', 'with', phrase)\n",
    "    phrase = re.sub(r'w/o', 'without', phrase)\n",
    "    phrase = re.sub(r'\\bive\\b', 'i have', phrase)\n",
    "    phrase = re.sub(r'\\blets\\b', 'let us', phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r'n\\'t', ' not', phrase)\n",
    "    phrase = re.sub(r'\\'re', ' are', phrase)\n",
    "    phrase = re.sub(r'\\'s', ' is', phrase)\n",
    "    phrase = re.sub(r'\\'d', ' would', phrase)\n",
    "    phrase = re.sub(r'\\'ll', ' will', phrase)\n",
    "    phrase = re.sub(r'\\'t', ' not', phrase)\n",
    "    phrase = re.sub(r'\\'ve', ' have', phrase)\n",
    "    phrase = re.sub(r'\\'m', ' am', phrase)\n",
    "    return phrase\n",
    "\n",
    "#clean data: removing symbols/formatting characters, making all characters lower case, removing contractions\n",
    "\n",
    "for i in range(len(review_text)): \n",
    "    review_text[i]=decontracted(review_text[i].lower())\n",
    "    review_text[i]=re.sub(r'(http\\S+|\\[.*?\\])|\\\\n|\\W|(\\$\\S+)|\\d|[^\\x00-\\x7A]', ' ', review_text[i])\n",
    "    \n",
    "#convert boolean strings to int for reviews (1 means a positive review, 0 means negative)\n",
    "rating_encoded=[]\n",
    "for i in review_rating:\n",
    "    rating_encoded.append(int(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3912, 21) {'num_reviews': 1, 'review_score': 5, 'review_score_desc': 'Mixed', 'total_positive': 3968, 'total_negative': 5298, 'total_reviews': 9266}\n"
     ]
    }
   ],
   "source": [
    "print(review_data.shape, review_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Converting the reviews to word vectors\n",
    "\n",
    "#initialize a counter object\n",
    "word_count =Counter()\n",
    "\n",
    "#For each review in our file, we update our counter with any new words and increase the count of words already seen.\n",
    "for review in review_text:\n",
    "    word_count.update(review.split())\n",
    "    \n",
    "#identifying words only used once, removing most common words (like the, and, a)\n",
    "top_word_list = word_count.most_common(5)\n",
    "one_appearance_word_list = [w for w in word_count if word_count[w] == 1]\n",
    "#len(one_appearance_word_list)\n",
    "#len(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing useless words, \n",
    "\n",
    "#initializing a regex pattern for removing words\n",
    "one_word_regex=''\n",
    "\n",
    "#a single regex expression is created by concatenating each word in the list of words to remove\n",
    "for word in one_appearance_word_list:\n",
    "    one_word_regex+='\\\\b'+ word + '\\\\b|'\n",
    "    \n",
    "for word, count in top_word_list:  \n",
    "    one_word_regex+='\\\\b'+ word + '\\\\b|'\n",
    "pattern=re.compile(one_word_regex.rstrip('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text_trimmed=[]\n",
    "#removing words that appear one time, or appear so often that they contain no information\n",
    "for i in range(len(review_text)):\n",
    "    review_text_trimmed.append(re.sub(pattern,'', review_text[i]))\n",
    "    review_text_trimmed[i]=re.sub('\\s{2,}',' ', review_text_trimmed[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " game was abandoned by devs is it worth it at its current state no only glimpse hope were promised updates that never came they will not deliver promised updates because game underperforms but game underperforms because they did not deliver promised updates they said they were going focus on other titles good for them i am going focus on other companies that do not abandon unfinished games  \n",
      " the game was abandoned by devs  is it worth it at its current state  no  the only glimpse of hope were the promised updates that never came  they will not deliver promised updates  because the game underperforms  but the game underperforms  because they did not deliver the promised updates  they said they were going to focus on other titles  good for them  i am going to focus on other companies that do not abandon unfinished games \n"
     ]
    }
   ],
   "source": [
    "print(review_text_trimmed[0], '\\n', review_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and save encoder for our reviews,\n",
    "encoder = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    review_text_trimmed, target_vocab_size=len(word_count))\n",
    "encoder.save_to_file('SteamData/SteamReviewVocab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=tfds.features.text.SubwordTextEncoder.load_from_file('SteamData/SteamReviewVocab')\n",
    "\n",
    "#encode the words \n",
    "encoded_reviews=[]\n",
    "for i in review_text_trimmed:\n",
    "    encoded_reviews.append(encoder.encode(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating tensorflow datasets for training\n",
    "def labeler(review, rating):\n",
    "    return review, rating\n",
    "#pairing the labels (good/bad game) with the encoded reviews\n",
    "encoded_review_rating_list=[]\n",
    "for i,j in enumerate(encoded_reviews):\n",
    "    encoded_review_dataset = tf.data.Dataset.from_tensors(j)\n",
    "    encoded_review_rating_list.append(encoded_review_dataset.map(lambda x: labeler(x,rating_encoded[i])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the list of review:score sets into a single tensor dataset.\n",
    "encoded_review_ratings = encoded_review_rating_list[0]\n",
    "for single_dataset in encoded_review_rating_list[1:]:\n",
    "    encoded_review_ratings=encoded_review_ratings.concatenate(single_dataset)\n",
    "\n",
    "#Shuffle the datasets to avoid any biases.\n",
    "buffer_size = len(encoded_reviews)\n",
    "all_labeled_data = encoded_review_ratings.shuffle(\n",
    "    buffer_size, reshuffle_each_iteration=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Split the encoded words into training and test datasets, take size amount of data that goes into the training set\n",
    "training_ratio=0.6\n",
    "take_size= round(len(encoded_reviews)*training_ratio)\n",
    "batch_size=30\n",
    "\n",
    "train_data = encoded_review_ratings.take(take_size)\n",
    "train_data = train_data.padded_batch(batch_size, padded_shapes=([None],()))\n",
    "\n",
    "test_data = encoded_review_ratings.skip(take_size)\n",
    "test_data = test_data.padded_batch(batch_size, padded_shapes=([None],()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, None, 16)          160352    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_7 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 150)               2550      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 100)               15100     \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 183,103\n",
      "Trainable params: 183,103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim=16\n",
    "learn_rate= 0.003\n",
    "#determining layers of our neural network\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(encoder.vocab_size, embedding_dim),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dense(150, activation='relu'),\n",
    "    layers.Dense(100, activation='relu'),\n",
    "    layers.Dense(50, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Received uninstantiated Loss class: <class 'tensorflow.python.keras.losses.MeanSquaredError'>\nPlease call loss \"\"classes before passing them to Model.compile.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-093b4aa93642>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m model.compile(optimizer=opt,\n\u001b[0;32m      6\u001b[0m               \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_eqn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m               metrics=['accuracy'])\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m history = model.fit(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;31m# Prepare list of loss functions, same size of model outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m     self.loss_functions = training_utils.prepare_loss_functions(\n\u001b[1;32m--> 409\u001b[1;33m         self.loss, self.output_names)\n\u001b[0m\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[0mtarget_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_target_tensor_for_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mprepare_loss_functions\u001b[1;34m(loss, output_names)\u001b[0m\n\u001b[0;32m   1453\u001b[0m     \u001b[0mloss_functions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_loss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1454\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1455\u001b[1;33m     \u001b[0mloss_functions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_loss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1457\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mloss_functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1453\u001b[0m     \u001b[0mloss_functions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_loss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1454\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1455\u001b[1;33m     \u001b[0mloss_functions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_loss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1457\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mloss_functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mget_loss_function\u001b[1;34m(loss)\u001b[0m\n\u001b[0;32m   1167\u001b[0m     raise ValueError(\n\u001b[0;32m   1168\u001b[0m         \u001b[1;34m'Received uninstantiated Loss class: {}\\nPlease call loss \"\"classes '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m         'before passing them to Model.compile.'.format(loss))\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m   \u001b[1;31m# Deserialize loss configuration, if needed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Received uninstantiated Loss class: <class 'tensorflow.python.keras.losses.MeanSquaredError'>\nPlease call loss \"\"classes before passing them to Model.compile."
     ]
    }
   ],
   "source": [
    "#set the optimizer and loss equation here, then compile and run the model.\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learn_rate)\n",
    "loss_eqn=tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "model.compile(optimizer=opt,\n",
    "              loss=loss_eqn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=30,\n",
    "    validation_data=test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the accuracy and losses for our training and validation sets.\n",
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss=history_dict['loss']\n",
    "val_loss=history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim((0.5,1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract TSV files of the word embeddings, to be used with the embedding project http://projector.tensorflow.org/\n",
    "import io\n",
    "\n",
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for num, word in enumerate(encoder.subwords):\n",
    "    vec = weights[num+1] # skip 0, it's padding.\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
